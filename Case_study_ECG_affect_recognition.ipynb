{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Case_study_ECG_affect_recognition.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOYm8rX9d2v1cxrXQHS14DA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FedotovD/WESAD_ECG_SSL/blob/main/Case_study_ECG_affect_recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVFnGFpOQ0F8"
      },
      "source": [
        "# Introduction\r\n",
        "\r\n",
        "This Python Notebook file will guide you through the process of affect recognition based on electrocardiographic data (ECG) using three approaches:\r\n",
        "\r\n",
        "\r\n",
        "1.   Classical: utilizing expert knowledge based features and training a Support Vector Machines (SVM) model for three class classification problem.\r\n",
        "2.   Fully-supervised deep learning based: utilizing raw signal to train a Convolutional Neural Network (CNN) from scratch, directly on target data.\r\n",
        "3.   Self-supervised deep learning based: utilizing model pretrained on transformed dataset using transfer learning. \r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eS3e57xpTgbH"
      },
      "source": [
        "# Dataset\r\n",
        "\r\n",
        "We will use WESAD (Wearable Stress and Affect Detection) database, collected by Corporate Research Group of Robert Bosch GmbH. It is available for download [here](https://archive.ics.uci.edu/ml/datasets/WESAD+%28Wearable+Stress+and+Affect+Detection%29). For more detailed information about the dataset, please refer to the [original paper](https://www.eti.uni-siegen.de/ubicomp/papers/ubi_icmi2018.pdf) by Philip Schmidt, Attila Reiss, Robert DÃ¼richen, Claus Marberger, Kristof Van Laerhoven.\r\n",
        "\r\n",
        "## Preprocessing\r\n",
        "For classical approach we have extracted the following features:\r\n",
        "\r\n",
        "For deep-learning based approach we have downsampled data to 256Hz with accordance to [the recent paper](https://arxiv.org/pdf/2002.03898.pdf) and to facilitate ease of the model training.\r\n",
        "\r\n",
        "Further preprocessing (e.g. train / dev / validation split, data normalization) is done in corresponding code cells below.\r\n",
        "\r\n",
        "Let's start by importing necessery libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCJkd-FpWEEn"
      },
      "source": [
        "import os\r\n",
        "import gc\r\n",
        "\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import tensorflow as tf\r\n",
        "\r\n",
        "from tensorflow import keras\r\n",
        "from tensorflow.keras import layers\r\n",
        "\r\n",
        "from sklearn.preprocessing import StandardScaler\r\n",
        "from sklearn.svm import SVC\r\n",
        "from sklearn.metrics import accuracy_score, recall_score, f1_score, confusion_matrix\r\n",
        "from sklearn.model_selection import LeaveOneGroupOut,LeavePGroupsOut\r\n",
        "\r\n",
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cifJJ7cdWxcr"
      },
      "source": [
        "Let's now download the data from a git repository."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgFem1unW8H4"
      },
      "source": [
        "# Remove the folder if already exists\r\n",
        "if os.path.exists('WESAD_ECG_SSL'):\r\n",
        "    ! rm -rf WESAD_ECG_SSL\r\n",
        "\r\n",
        "# Clone the git repo\r\n",
        "! git clone --quiet https://github.com/FedotovD/WESAD_ECG_SSL.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szyGs7xeoQZm"
      },
      "source": [
        "# Classical approach\r\n",
        "\r\n",
        "We start by calculating a baseline for this data. Let's read the expert knowledge based features and print several rows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJh6GvIeXM8d"
      },
      "source": [
        "ecg_expert = pd.read_csv('WESAD_ECG_SSL/data/ECG_expert_features.csv', index_col=0)\r\n",
        "ecg_expert.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syY0UP55ojEj"
      },
      "source": [
        "We now split it into train and test, then normalize the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agcg19EKtBWN"
      },
      "source": [
        "def class_to_number(labels):\r\n",
        "    # This function converts classes from categories to numbers, e.g. \"Base\" to 0, \"Fun\" to 1, etc.\r\n",
        "\r\n",
        "    classes = np.unique(labels)\r\n",
        "\r\n",
        "    for c in range(len(classes)):\r\n",
        "        labels[labels == classes[c]] = c\r\n",
        "    \r\n",
        "    return np.array(labels, dtype=int)\r\n",
        "\r\n",
        "def one_hot_encoding(labels, num_classes):\r\n",
        "    # This function performs one hot encoding for labels\r\n",
        "\r\n",
        "    if type(labels[0]) == str:\r\n",
        "        labels = class_to_number(labels)\r\n",
        "\r\n",
        "    elif type(labels[0]) == float:\r\n",
        "        labels = int(labels)\r\n",
        "\r\n",
        "    Y = np.eye(num_classes)[labels]\r\n",
        "\r\n",
        "    return Y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1pP8JmLoDWF"
      },
      "source": [
        "# Define train and test sets\r\n",
        "train_participants = ['S2', 'S3', 'S4', 'S5', 'S6', 'S7', 'S8', 'S9', 'S10', 'S11', 'S13', 'S14']\r\n",
        "test_participants  = ['S15', 'S16', 'S17']\r\n",
        "\r\n",
        "# Subsample indices for each set\r\n",
        "train_idx = ecg_expert[ecg_expert.pid.isin(train_participants)].index\r\n",
        "test_idx = ecg_expert[ecg_expert.pid.isin(test_participants)].index\r\n",
        "\r\n",
        "# Split data\r\n",
        "X_train = ecg_expert.loc[train_idx].iloc[:,2:].to_numpy()    # iloc[:,2:] drops first two columns (pid and lab)\r\n",
        "y_train = ecg_expert.loc[train_idx].lab.to_numpy()\r\n",
        "y_train = class_to_number(y_train)\r\n",
        "\r\n",
        "X_test  = ecg_expert.loc[test_idx].iloc[:,2:].to_numpy()\r\n",
        "y_test  = ecg_expert.loc[test_idx].lab.to_numpy()\r\n",
        "y_test  = class_to_number(y_test)\r\n",
        "\r\n",
        "# Normalize data\r\n",
        "scaler = StandardScaler()\r\n",
        "X_train_norm = scaler.fit_transform(X_train)\r\n",
        "X_test_norm  = scaler.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDsoBbHsv7in"
      },
      "source": [
        "Now we are ready to train the classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnwB8zSKoBgb"
      },
      "source": [
        "# Define classifier (SVM for classification) with default parameters\r\n",
        "clf = SVC()\r\n",
        "\r\n",
        "# Fit it to train data\r\n",
        "clf.fit(X_train_norm, y_train)\r\n",
        "\r\n",
        "# Make predictions\r\n",
        "pred_classical = clf.predict(X_test_norm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzsRJn-EzZ2Z"
      },
      "source": [
        "Let's check the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VcC4TDtvDum"
      },
      "source": [
        "print('Accuracy\\t{}'.format(np.round(accuracy_score(pred_classical, y_test),3)))\r\n",
        "print('F1 score\\t{}'.format(np.round(f1_score(pred_classical, y_test, average='macro'),3)))\r\n",
        "print('UAR\\t\\t{}'.format(np.round(recall_score(pred_classical, y_test, average='macro'),3)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXHEht4B0dnU"
      },
      "source": [
        "Can we do better? Sure! Let's try deep learning based approach"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdJ_9vtl044z"
      },
      "source": [
        "# Fully supervised deep learning based approach\r\n",
        "\r\n",
        "At this step we will use CNN to recognize affect. The architecture is proposed [here](https://https://arxiv.org/pdf/2002.03898.pdf). We will use TensorFlow and Keras to create, train and test our model.\r\n",
        "\r\n",
        "We read the data at first. Note that for the data of each participant a separate file is assigned. Therefore, the procedure is slightly different here.\r\n",
        "\r\n",
        "Also, in order to avoid overfitting during training, we need an additional development set. We will check the performance on it while training to decide when to stop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fND8v8GR3hK3"
      },
      "source": [
        "def read_data(participants):\r\n",
        "    # This function reads the data according to participants list taken as an argument\r\n",
        "\r\n",
        "    X = []\r\n",
        "\r\n",
        "    for filename in train_participants:\r\n",
        "        temp = pd.read_csv('WESAD_ECG_SSL/data/{}.csv'.format(filename), index_col=0)\r\n",
        "        X.append(temp)\r\n",
        "    \r\n",
        "    X = pd.concat(X, axis=0)\r\n",
        "    y = X.lab.to_numpy()\r\n",
        "    y = class_to_number(y)\r\n",
        "    X = X.iloc[:,2:].to_numpy()\r\n",
        "    \r\n",
        "    return X, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q395sWTBoCLV"
      },
      "source": [
        "# Define train and test sets\r\n",
        "train_participants = ['S2', 'S3', 'S4', 'S5', 'S6', 'S7', 'S8', 'S9', 'S10']\r\n",
        "devel_participants = ['S11', 'S13', 'S14']\r\n",
        "test_participants  = ['S15', 'S16', 'S17']\r\n",
        "\r\n",
        "\r\n",
        "# Read files for train and test participants\r\n",
        "X_train, y_train = read_data(train_participants)\r\n",
        "X_devel, y_devel = read_data(devel_participants)\r\n",
        "X_test, y_test = read_data(test_participants)\r\n",
        "\r\n",
        "\r\n",
        "# Normalize data\r\n",
        "scaler = StandardScaler()\r\n",
        "X_train_norm = scaler.fit_transform(X_train)\r\n",
        "X_devel_norm = scaler.transform(X_devel)\r\n",
        "X_test_norm  = scaler.transform(X_test)\r\n",
        "\r\n",
        "\r\n",
        "# Convert labels to one-hot\r\n",
        "y_train_one_hot = one_hot_encoding(y_train, 3)\r\n",
        "y_devel_one_hot = one_hot_encoding(y_devel, 3)\r\n",
        "y_test_one_hot = one_hot_encoding(y_test, 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcILewGm5NP4"
      },
      "source": [
        "Now we have the data ready. Let's get to modeling!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlOAA-eXqvpd"
      },
      "source": [
        "def create_graph(input_shape=2560, num_classes=3):\r\n",
        "    inputs = keras.Input(shape=(input_shape,1))\r\n",
        "\r\n",
        "    # Conv1\r\n",
        "    x = layers.Conv1D(filters=32, kernel_size=32, strides=1)(inputs)\r\n",
        "    x = layers.BatchNormalization()(x)\r\n",
        "    x = layers.Activation(layers.LeakyReLU())(x)\r\n",
        "\r\n",
        "    x = layers.Conv1D(filters=32, kernel_size=32, strides=1)(x)\r\n",
        "    x = layers.BatchNormalization()(x)\r\n",
        "    x = layers.Activation(layers.LeakyReLU())(x)\r\n",
        "\r\n",
        "    x = layers.MaxPooling1D(pool_size=8, strides=2)(x)\r\n",
        "\r\n",
        "    # Conv2\r\n",
        "    x = layers.Conv1D(filters=64, kernel_size=16, strides=1)(x)\r\n",
        "    x = layers.BatchNormalization()(x)\r\n",
        "    x = layers.Activation(layers.LeakyReLU())(x)\r\n",
        "\r\n",
        "    x = layers.Conv1D(filters=64, kernel_size=16, strides=1)(x)\r\n",
        "    x = layers.BatchNormalization()(x)\r\n",
        "    x = layers.Activation(layers.LeakyReLU())(x)\r\n",
        "\r\n",
        "    x = layers.GlobalMaxPooling1D()(x)\r\n",
        "    x = layers.Flatten()(x)\r\n",
        "\r\n",
        "    # Head\r\n",
        "    x = layers.Dense(128)(x)\r\n",
        "    x = layers.Dropout(0.5)(x)\r\n",
        "    x = layers.Activation(layers.LeakyReLU())(x)\r\n",
        "    outputs = layers.Dense(num_classes, 'softmax')(x)\r\n",
        "  \r\n",
        "    return inputs, outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWEgXE3F5v6k"
      },
      "source": [
        "# Create a model\r\n",
        "inputs, outputs = create_graph(input_shape=2560, num_classes=3)\r\n",
        "model_fully_supervised = keras.Model(inputs, outputs)\r\n",
        "\r\n",
        "# Define an optimizer and callbacks (to stop the training when the development loss does not decrease)\r\n",
        "opt = keras.optimizers.Adam(lr=0.001)\r\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\r\n",
        "\r\n",
        "# Compile and fit the model\r\n",
        "model_fully_supervised.compile(optimizer=opt, loss='categorical_crossentropy')\r\n",
        "history_fs = model_fully_supervised.fit(X_train_norm, y_train_one_hot, validation_data=(X_devel_norm, y_devel_one_hot), callbacks=[callback], epochs=100, batch_size=32, verbose=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHcuMHTYUn8l"
      },
      "source": [
        "Let's plot the graphs of train and validation loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GA5PP-qr-qqV"
      },
      "source": [
        "plt.plot(history_fs.history['loss'])\r\n",
        "plt.plot(history_fs.history['val_loss'])\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m81SazVUUu8q"
      },
      "source": [
        "Next, let's make a prediction for text set and calculate metric values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwY71y0P6tZj"
      },
      "source": [
        "pred_fully_supervised = model_fully_supervised.predict(X_test_norm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxocEKR47o1p"
      },
      "source": [
        "print('Accuracy\\t{}'.format(np.round(accuracy_score(np.argmax(pred_fully_supervised, axis=1), y_test),3)))\r\n",
        "print('F1 score\\t{}'.format(np.round(f1_score(np.argmax(pred_fully_supervised, axis=1), y_test, average='macro'),3)))\r\n",
        "print('UAR\\t\\t{}'.format(np.round(recall_score(np.argmax(pred_fully_supervised, axis=1), y_test, average='macro'),3)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8qe7ixx_pM9"
      },
      "source": [
        "# Self supervised + transfer learning\r\n",
        "\r\n",
        "Next, let's use self-supervised approach to learn representations from data (pretext model) and then apply transfer learning to use these representations for the target task (downstream model). \r\n",
        "\r\n",
        "First, we will need a way to create a proxy task for the model to train on. In our case it will be classification of transformation type applied to the signal (if any). \r\n",
        "\r\n",
        "We apply one of these transormations: horizontal flip, addition of noise, scaling by a certain factor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66fAvxef82nc"
      },
      "source": [
        "# Three functions below are taken from source code of the autors of this paper:\r\n",
        "# https://arxiv.org/pdf/2002.03898.pdf\r\n",
        "\r\n",
        "def hor_filp(signal):\r\n",
        "    \"\"\" \r\n",
        "    flipped horizontally \r\n",
        "    \"\"\"\r\n",
        "    hor_flipped = np.flip(signal)\r\n",
        "    return hor_flipped\r\n",
        "\r\n",
        "def add_noise_with_SNR(signal, noise_amount):\r\n",
        "    \"\"\" \r\n",
        "    adding noise\r\n",
        "    created using: https://stackoverflow.com/a/53688043/10700812 \r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "    target_snr_db = noise_amount #20\r\n",
        "    x_watts = signal ** 2                       # Calculate signal power and convert to dB \r\n",
        "    sig_avg_watts = np.mean(x_watts)\r\n",
        "    sig_avg_db = 10 * np.log10(sig_avg_watts)   # Calculate noise then convert to watts\r\n",
        "    noise_avg_db = sig_avg_db - target_snr_db\r\n",
        "    noise_avg_watts = 10 ** (noise_avg_db / 10)\r\n",
        "    mean_noise = 0\r\n",
        "    noise_volts = np.random.normal(mean_noise, np.sqrt(noise_avg_watts), len(x_watts))     # Generate an sample of white noise\r\n",
        "    noised_signal = signal + noise_volts        # noise added signal\r\n",
        "\r\n",
        "    return noised_signal \r\n",
        "\r\n",
        "def scaled(signal, factor):\r\n",
        "    \"\"\"\"\r\n",
        "    scale the signal\r\n",
        "    \"\"\"\r\n",
        "    scaled_signal = signal * factor\r\n",
        "    return scaled_signal\r\n",
        "\r\n",
        "def transform_data(X):\r\n",
        "    # This function performes defined above transformations and creates artificial labels.\r\n",
        "\r\n",
        "    X_noised = np.zeros(X.shape)\r\n",
        "    X_scaled = np.zeros(X.shape)\r\n",
        "    X_flipped = np.zeros(X.shape)\r\n",
        "\r\n",
        "    for i in range(X.shape[0]):\r\n",
        "        X_noised[i,:] = add_noise_with_SNR(X[i,:], 20)\r\n",
        "        X_scaled[i,:] = scaled(X[i,:], 1.1)\r\n",
        "        X_flipped[i,:] = hor_filp(X[i,:])\r\n",
        "    \r\n",
        "    full_data = np.concatenate((X, X_noised, X_scaled, X_flipped))\r\n",
        "    full_labels = np.concatenate((np.full(X.shape[0], 0),\r\n",
        "                                  np.full(X.shape[0], 1),\r\n",
        "                                  np.full(X.shape[0], 2),\r\n",
        "                                  np.full(X.shape[0], 3)))\r\n",
        "    \r\n",
        "    return full_data, full_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJuGkqQsWLCu"
      },
      "source": [
        "We will directly use normalized data from previous task. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fn700LLu7r_N"
      },
      "source": [
        "# Transform data and get artificial labels\r\n",
        "X_train_transformed, y_train_transformed = transform_data(X_train_norm)\r\n",
        "X_devel_transformed, y_devel_transformed = transform_data(X_devel_norm)\r\n",
        "\r\n",
        "# Perform one hot encoding for labels\r\n",
        "y_train_transformed_one_hot = one_hot_encoding(y_train_transformed, 4)\r\n",
        "y_devel_transformed_one_hot = one_hot_encoding(y_devel_transformed, 4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sK7WlPWWZN1"
      },
      "source": [
        "Let's create a pretext model for transformation classification task and train it. In this case the number of classes is 4: [original, noised, scaled, flipped] signal. We use the same pipeline as before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bE0PB0pUAu7B"
      },
      "source": [
        "# Create a model\r\n",
        "inputs, outputs = create_graph(input_shape=2560, num_classes=4)\r\n",
        "model_pretext = keras.Model(inputs, outputs)\r\n",
        "\r\n",
        "# Define an optimizer and callbacks (to stop the training when the development loss does not decrease)\r\n",
        "opt = keras.optimizers.Adam(lr=0.001)\r\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\r\n",
        "\r\n",
        "# Compile and fit the model\r\n",
        "model_pretext.compile(optimizer=opt, loss='categorical_crossentropy')\r\n",
        "history = model_pretext.fit(X_train_transformed, y_train_transformed_one_hot, validation_data=(X_devel_transformed, y_devel_transformed_one_hot),\r\n",
        "                            callbacks=[callback], epochs=1, batch_size=32, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcFjeLYPW1hT"
      },
      "source": [
        "Let's plot the graphs of train and validation loss here as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvftO6NRCfBi"
      },
      "source": [
        "plt.plot(history.history['loss'])\r\n",
        "plt.plot(history.history['val_loss'])\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEdyT24VW4Gw"
      },
      "source": [
        "Now, we don't need to calculate a metric here, as it is a proxy task.\r\n",
        "\r\n",
        "We will further use this model in the following way: on top of the last pooling (global average pooling) we will stack a hidden dense layer and then output layer with softmax activation. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOW8QsY3Egx8"
      },
      "source": [
        "# Stack the layers\r\n",
        "x = layers.Dense(128)(model_pretext.layers[-5].output)\r\n",
        "x = layers.Dropout(0.5)(x)\r\n",
        "x = layers.Activation(layers.LeakyReLU())(x)\r\n",
        "outputs_downstream = layers.Dense(3, 'softmax')(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2J5Ms5YGXdkZ"
      },
      "source": [
        "Now we are ready to construct our downstream model. Let's do this and train the model afterwards."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGPPqvagFTDi"
      },
      "source": [
        "# Create a model\r\n",
        "model_downstream = keras.Model(inputs, outputs_downstream)\r\n",
        "for i in range(len(model_downstream.layers)-5):\r\n",
        "    model_downstream.layers[i].trainable = False\r\n",
        "\r\n",
        "# Define an optimizer and callbacks (to stop the training when the development loss does not decrease)\r\n",
        "opt = keras.optimizers.Adam(lr=0.001)\r\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\r\n",
        "\r\n",
        "# Compile and fit the model\r\n",
        "model_downstream.compile(optimizer=opt, loss='categorical_crossentropy')\r\n",
        "history = model_downstream.fit(X_train_norm, y_train_one_hot, validation_data=(X_devel_norm, y_devel_one_hot), callbacks=[callback], epochs=100, batch_size=32, verbose=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rah0Yb4uX2wg"
      },
      "source": [
        "We plot loss graphs here as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNP5Xm_xFTnk"
      },
      "source": [
        "plt.plot(history.history['loss'])\r\n",
        "plt.plot(history.history['val_loss'])\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgZstoJzX7fp"
      },
      "source": [
        "And now we are ready for the final prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F41i4thxGi0S"
      },
      "source": [
        "pred_downstream = model_downstream.predict(X_test_norm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8oKm77sBG2t-"
      },
      "source": [
        "print('Accuracy\\t{}'.format(np.round(accuracy_score(np.argmax(pred_downstream, axis=1), y_test),3)))\r\n",
        "print('F1 score\\t{}'.format(np.round(f1_score(np.argmax(pred_downstream, axis=1), y_test, average='macro'),3)))\r\n",
        "print('UAR\\t\\t{}'.format(np.round(recall_score(np.argmax(pred_downstream, axis=1), y_test, average='macro'),3)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWtpNe0uYAPS"
      },
      "source": [
        "Let's compare the results of these two deep learning based approaches."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQoWMlYBHH_I"
      },
      "source": [
        "print('Self supervised UAR\\t{}'.format(np.round(recall_score(np.argmax(pred_downstream, axis=1), y_test, average='macro'),3)))\r\n",
        "print('Fully supervised UAR\\t{}'.format(np.round(recall_score(np.argmax(pred_fully_supervised, axis=1), y_test, average='macro'),3)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7t6jEIDvPMS"
      },
      "source": [
        "# Further task\r\n",
        "Now, in order to get a clearer understanding if self-supervised learning helps to obtain better predictions, we need to perform cross-validation. As we have only 15 participants, a good idea may be to use LOGO (Leave-One-Group-Out) cross-validation. In this case each group will contain samples of one participant. This data will be held out for one experiment and serve as test set. We do this procedure 15 times (one time for each participant) and then average the performance or concatenate prediction vectors and calculating the metric afterwards. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLUPsw5tVqzJ"
      },
      "source": [
        "# Read saved results\r\n",
        "self_supervised = pd.read_csv('WESAD_ECG_SSL/results_cv/self_supervised.csv', index_col=0)\r\n",
        "fully_supervised = pd.read_csv('WESAD_ECG_SSL/results_cv/fully_supervised.csv', index_col=0)\r\n",
        "classical = pd.read_csv('WESAD_ECG_SSL/results_cv/classical.csv', index_col=0)\r\n",
        "\r\n",
        "# Extract mean over cross validation partitions\r\n",
        "self_supervised_mean = np.mean(self_supervised, axis=0)\r\n",
        "fully_supervised_mean = np.mean(fully_supervised, axis=0)\r\n",
        "classical_mean = np.mean(classical, axis=0)\r\n",
        "\r\n",
        "x = np.arange(3)\r\n",
        "\r\n",
        "# Plot the figure\r\n",
        "plt.figure(figsize=(6,8))\r\n",
        "plt.bar(x-0.2, classical_mean, width=0.2, color=(0.835,0.047,0), label='Classical')\r\n",
        "plt.bar(x, fully_supervised_mean, width=0.2, color=(0.055,0.471,0.773), label='Fully supervised')\r\n",
        "plt.bar(x+0.2, self_supervised_mean, width=0.2, color=(0.404,0.705,0.098), label='Self supervised')\r\n",
        "\r\n",
        "plt.ylim([0.4,1])\r\n",
        "plt.xticks([0,1,2], ['Accuracy', 'F1 score', 'UAR'], size=12)\r\n",
        "\r\n",
        "plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05), ncol=3)\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRtlX9eCZTLW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}